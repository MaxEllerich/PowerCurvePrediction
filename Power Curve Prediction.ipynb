{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning approach for predicting power curves of manufacturing processes\n",
    "\n",
    "This Version (v1.0) was published alongside the homonymous <a href=\"url\">CIRP paper</a>.\n",
    "The program is part of an on-going research project of the Laboratory for Machine Tools and Production Engineering WZL, RWTH Aachen University, Aachen, Germany. Further adaptions and other use cases are foreseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Overview\n",
    "* __data_preparation:__ Import data from .csv file; One hot encoding; Shuffle series of measurements. <br>\n",
    "* __seperate_Xy:__ Seperates the input data (printer settings) from the wattage measurements.<br>\n",
    "* __normalization:__ Normalize the measurements to make the NN learn better. <br>\n",
    "* __DNN_model:__ Creates an sequential model of the given input settings.<br>\n",
    "* __train_dnn:__ Trains the NN with the given settings.<br>\n",
    "* __plot_model_history:__ Plots the accuracy and loss values for train and validation data of the model.<br>\n",
    "* __descale:__ Takes the predicted values of the noralized inputs and converts them back to the original scale.<br>\n",
    "* __rmse_mae:__ Takes the predicted values and calculates the rmses and maes of each measurement. <br>\n",
    "* __prediction_graph:__ Plots predictions together with the original data.<br>\n",
    "* __get_X_labels:__ Converts the one hot encoding back to normal job labels.<br>\n",
    "* __model_1:__ Creates an DNN via the integraded approach and makes test predictions.<br>\n",
    "* __y_single_value_filter:__ Returns all measurements for a specific point of time.<br>\n",
    "* __model_2:__ Creates models for each measurement timestep and returns predictions on these models on the test data.<br>\n",
    "* __One Run --> Excel:__ Makes Predictions on both models for shuffled data various times.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from math import exp\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras import metrics\n",
    "import warnings\n",
    "import time\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)  \n",
    "# Set global variables\n",
    "num_of_printers = 5\n",
    "y_position_in_csv, attributes_of_part = num_of_printers+5,num_of_printers+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Auftrag</th>\n",
       "      <th>Geometrie</th>\n",
       "      <th>DruckerID</th>\n",
       "      <th>Volumen</th>\n",
       "      <th>Filamentdichte</th>\n",
       "      <th>Wirkleistung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quader</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32.029920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Quader</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>52.229870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Quader</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>98.925456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Auftrag Geometrie  DruckerID  Volumen  Filamentdichte  Wirkleistung\n",
       "0        1    Quader          2      8.0              10     32.029920\n",
       "1        1    Quader          2      8.0              10     52.229870\n",
       "2        1    Quader          2      8.0              10     98.925456"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('print_data.csv', header=0, sep=';')\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13032 entries, 0 to 13031\n",
      "Data columns (total 6 columns):\n",
      "Auftrag           13032 non-null int64\n",
      "Geometrie         13032 non-null object\n",
      "DruckerID         13032 non-null int64\n",
      "Volumen           13032 non-null float64\n",
      "Filamentdichte    13032 non-null int64\n",
      "Wirkleistung      13032 non-null float64\n",
      "dtypes: float64(2), int64(3), object(1)\n",
      "memory usage: 611.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation():\n",
    "    \n",
    "    origin = dataset.pop('Geometrie')\n",
    "    \n",
    "    dataset['Quader'] = (origin == 'Quader')*1.0\n",
    "    dataset['Kegel'] = (origin == 'Kegel')*1.0\n",
    "\n",
    "    origin = dataset.pop('DruckerID')\n",
    "    \n",
    "    for i in range(num_of_printers):\n",
    "        dataset['Drucker_%s'%(i+1)] = (origin == (i+1))*1.0\n",
    "\n",
    "    dataset.shape[0]\n",
    "    dataset = dataset[['Auftrag','Filamentdichte','Drucker_1','Drucker_2','Drucker_3','Drucker_4','Drucker_5','Volumen','Quader','Kegel', 'Wirkleistung']]    \n",
    "    \n",
    "    def get_y(dataset):\n",
    "        messwert = []\n",
    "        y = []\n",
    "        for i in range(dataset.shape[0]):\n",
    "            def new_part ():\n",
    "                messwert.append(dataset.iloc[i,y_position_in_csv])\n",
    "            def add_to_parts():\n",
    "                y.append(messwert)\n",
    "\n",
    "            if i == 0:\n",
    "                new_part() \n",
    "            elif dataset.iloc[i, 0] == dataset.iloc[i-1, 0]:\n",
    "                messwert.append(dataset.iloc[i,y_position_in_csv])\n",
    "                if i == dataset.shape[0] - 1:\n",
    "                    add_to_parts()\n",
    "            else:\n",
    "                add_to_parts()\n",
    "                messwert = []\n",
    "                new_part()\n",
    "        return y\n",
    "        \n",
    "    y_data = get_y(dataset)\n",
    "\n",
    "    datenmenge = len(y_data)\n",
    "    longest_print = max(y_data, key=len)\n",
    "    longest_print = len(longest_print)\n",
    "\n",
    "    for y in y_data:\n",
    "        if len(y) < longest_print:\n",
    "            to_go = longest_print-len(y)\n",
    "            for z in range(to_go):\n",
    "                y.append(0)\n",
    "\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    X_dataset = dataset.drop(['Wirkleistung','Auftrag'], axis=1)\n",
    "    X_dataset = X_dataset.drop_duplicates()\n",
    "    X_data = np.array(X_dataset)\n",
    "    \n",
    "    all_data = X_data.tolist()\n",
    "    \n",
    "    for i in range(len(X_data)):\n",
    "        for k in range(len(y_data[i])):\n",
    "            all_data[i].append(y_data[i][k])\n",
    "            \n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    all_data = np.array(all_data, dtype=\"f\")\n",
    "    return all_data, longest_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_Xy(all_data):\n",
    "    X_data=[]\n",
    "    y_data =[]\n",
    "    for i in range(len(all_data)):\n",
    "        X_data.append(all_data[i][:y_position_in_csv-1])\n",
    "        y_data.append(all_data[i][y_position_in_csv-1:])\n",
    "    y_data = np.array(y_data)\n",
    "    X_data = np.array(X_data)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(all_data, scaler):\n",
    "    X_data, y_data = seperate_Xy(all_data)\n",
    "    number_of_datapoints = len(X_data)\n",
    "    \n",
    "    if scale_mode == \"x_y_seperate\":\n",
    "        scaled_X_data  = scaler.fit_transform(X_data)\n",
    "        scaled_y_data  = scaler.fit_transform(y_data) \n",
    "\n",
    "    if scale_mode == \"x_y_together\": \n",
    "        all_scaled_data = scaler.fit_transform(all_data.transpose())\n",
    "        all_scaled_data = all_scaled_data.transpose()\n",
    "        scaled_X_data, scaled_y_data = seperate_Xy(all_scaled_data)\n",
    "\n",
    "    if scale_mode == \"x_only\":\n",
    "        scaled_X_data = scaler.fit_transform(X_data)\n",
    "        scaled_y_data = y_data\n",
    "\n",
    "    number_of_datapoints = len(X_data)\n",
    "\n",
    "    org_y_test = np.array(y_data[round(0.9*number_of_datapoints):]) #10 Prozent testdaten, 90 Prozent zum Trainieren des Models\n",
    "    org_X_test = np.array(X_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    X_train = np.array(scaled_X_data[:round(0.9*number_of_datapoints)])\n",
    "    X_test = np.array(scaled_X_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    y_train = np.array(scaled_y_data[:round(0.9*number_of_datapoints)])\n",
    "    y_test = np.array(scaled_y_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    print(\"#Trainingsdaten:\",X_train.shape[0])\n",
    "    print(\"#Testdaten:\",X_test.shape[0], \"\\n\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, org_X_test, org_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#Open a console, change to your working directory, and type: tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(input_layer_size, dense_layers, hidden_layer_size, output_layer_size, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_size, activation='relu',input_shape = (X_train.shape[1:])))\n",
    "    \n",
    "    for l in range(dense_layers):\n",
    "        model.add(Dense(hidden_layer_size, activation= 'relu'))\n",
    "        #model.add(BatchNormalization(axis=1))\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_layer_size))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer = optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DNN(model, X_train, y_train, epochs, verbose,NAME):\n",
    "    \n",
    "    tbCallBack = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size= (round(X_train.shape[0]/3)),\n",
    "                        validation_split = 0.1,\n",
    "                        epochs = epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=True)\n",
    "                        #callbacks = [tbCallBack])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history, NAME):\n",
    "    pyplot.figure(100)\n",
    "    pyplot.plot(history.history['loss'], label='training')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.title(NAME)\n",
    "    pyplot.figure(101)\n",
    "    pyplot.plot(history.history['acc'], label='training')\n",
    "    pyplot.plot(history.history['val_acc'], label='validation')\n",
    "    pyplot.legend()  \n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(predictions, upper_limit):\n",
    "    if scale_mode in [\"x_y_seperate\"]:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "    if scale_mode == \"x_y_together\":\n",
    "        predictions = np.concatenate((X_test, predictions), axis = 1) \n",
    "        all_data_but_pred = np.concatenate((X_train, y_train[:,:upper_limit]), axis = 1)\n",
    "        all_data_but_pred = np.concatenate((all_data_but_pred, predictions), axis =0)\n",
    "        test_predictions_rescaled = scaler.inverse_transform((all_data_but_pred.transpose()))\n",
    "        test_predictions_rescaled = test_predictions_rescaled.transpose()\n",
    "        predictions = test_predictions_rescaled[X_train.shape[0]:,attributes_of_part:] \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_mae(predictions,y_test,lower_limit, upper_limit, return_single_values):\n",
    "    rmses, maes = [], []\n",
    "\n",
    "    if return_single_values == True:\n",
    "        for i in range(predictions.shape[0]):\n",
    "            rmse = sqrt(mean_squared_error(predictions[i], org_y_test[i,lower_limit:upper_limit]))\n",
    "            mae = mean_absolute_error(predictions[i], org_y_test[i,lower_limit:upper_limit])\n",
    "            rmses.append(rmse)\n",
    "            maes.append(mae)  \n",
    "    if return_single_values == False:        \n",
    "        rmses = sqrt(mean_squared_error(predictions, org_y_test[:,lower_limit:upper_limit]))\n",
    "        maes = mean_absolute_error(predictions, org_y_test[:,lower_limit:upper_limit])\n",
    "    return rmses, maes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_graph(predictions, lower_limit, upper_limit):\n",
    "    for i in range(len(y_test)):\n",
    "        pyplot.figure(i)\n",
    "        pyplot.plot(predictions[i], 'r',label='Vorhersage')\n",
    "        pyplot.plot(org_y_test[i,lower_limit:upper_limit],label='Leistungskurve')\n",
    "        pyplot.legend()\n",
    "        pyplot.ylabel('Wirkleistung [W]')\n",
    "        pyplot.xlabel('Zeitschritte [min]')\n",
    "        pyplot.title(org_X_test[i])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_labels(X_values):\n",
    "    NAMES = []\n",
    "    for i in range(X_values.shape[0]):\n",
    "        for j in range(num_of_printers):\n",
    "            if X_values[i,j+1] == 1:\n",
    "                printer = \"Printer%s\"%(j+1)\n",
    "        if X_values[i,num_of_printers+2] == 1:\n",
    "            geometry = \"Quader\"\n",
    "        else:\n",
    "            geometry = \"Kegel\"\n",
    "        NAME = \"%s %s F%s V%s\" %(printer,geometry,X_values[i,0],X_values[i,num_of_printers+1])\n",
    "        NAMES.append(NAME)\n",
    "    return NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_01 - Integrated Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1 (input_layer_size,hidden_layers,hidden_layer_size,epochs,optimizer):\n",
    "    NAME = \"%s-inlay_%s-hlayers_%s-hsize_%s\"%(input_layer_size,hidden_layers,hidden_layer_size,int(time.time()))\n",
    "    lower_limit, upper_limit = 0, longest_print\n",
    "    input_layer_size = input_layer_size\n",
    "    hidden_layers = hidden_layers\n",
    "    hidden_layer_size = hidden_layer_size\n",
    "    output_layer_size = longest_print\n",
    "    optimizer = optimizer\n",
    "    batch_size = [round(X_train.shape[0]/3)]\n",
    "    verbose = 0\n",
    "    epochs = epochs\n",
    "    \n",
    "    print(\"Training model_1 (appx. 15s)...\")\n",
    "    model = DNN_model(input_layer_size, hidden_layers, hidden_layer_size, output_layer_size,optimizer)\n",
    "    model, history = train_DNN(model,X_train, y_train, epochs, verbose,NAME)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction = descale(prediction, upper_limit) \n",
    "    \n",
    "    return model, history, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_02 - Multi Net Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_single_value_filter(y_data, timestep):\n",
    "    y_single_value = y_data.transpose()[timestep]\n",
    "    y_single_value = np.array([y_single_value])\n",
    "    y_single_value = y_single_value.transpose()\n",
    "    return y_single_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_02(model,X_train, y_train, epochs, verbose):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=(round(X_train.shape[0]/3)),\n",
    "                        validation_split = 0.1,\n",
    "                        epochs = epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=True)\n",
    "    prediction = model.predict(X_test)\n",
    "    return prediction, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(input_layer_size, hidden_layers, hidden_layer_size, epochs, optimizer, upper_limit): \n",
    "    NAME = \"%s-inlay_%s-hlayers_%s-hsize_%s\"%(input_layer_size,hidden_layers,hidden_layer_size,int(time.time()))\n",
    "    lower_limit, upper_limit = 0, upper_limit\n",
    "    input_layer_size = input_layer_size\n",
    "    hidden_layers = hidden_layers\n",
    "    hidden_layer_size = hidden_layer_size\n",
    "    output_layer_size = 1\n",
    "    optimizer = optimizer\n",
    "    epochs = epochs\n",
    "    verbose = 0\n",
    "    \n",
    "    model = DNN_model (input_layer_size, hidden_layers, hidden_layer_size, output_layer_size, optimizer)\n",
    "    predictions = np.array([[0]]*X_test.shape[0]) \n",
    "    print(\"Training model_2 (appx. 5min)\",end=\"\")\n",
    "    for y_stelle in range(lower_limit,upper_limit,1): \n",
    "        print(\".\", end=\"\")\n",
    "        y_single_train = y_single_value_filter(y_train,y_stelle)\n",
    "        y_single_test = y_single_value_filter(y_test, y_stelle)\n",
    "\n",
    "        model, history = train_DNN(model, X_train, y_single_train,epochs, verbose,NAME)\n",
    "        prediction = model.predict(X_test)\n",
    "        predictions = np.concatenate((predictions, prediction), axis=1)\n",
    "\n",
    "    predictions = np.delete(predictions,0,1) \n",
    "    predictions_descaled = descale(predictions, upper_limit)\n",
    "\n",
    "    return predictions_descaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Run -->  Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_excel(runs):\n",
    "    \n",
    "    from threading import Thread\n",
    "\n",
    "    keras.optimizers.Adam(lr=0.0001)\n",
    "    #keras.optimizers.RMSprop(lr=0.001)\n",
    "    batch_size = (round(X_train.shape[0]/3))\n",
    "    optimizer = 'adam'\n",
    "    codes = [666,999]\n",
    "    writer = pd.ExcelWriter('run_%s.xlsx'%(runs+1))\n",
    "    df_model = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    model_m1, history_m1, predictions_m1 = model_1(16,2,400,1000, optimizer)\n",
    "    rmse_m1, mae_m1 = rmse_mae(predictions_m1, y_test,0, longest_print, True) \n",
    "    rmse_mae_m1 = []\n",
    "    \n",
    "    predictions_m2 = model_2(64,1,64,100, optimizer, longest_print)\n",
    "    rmse_m2, mae_m2 = rmse_mae(predictions_m2, y_test, 0, longest_print, True)\n",
    "    rmse_mae_m2 = []\n",
    "    \n",
    "    for i in range(len(rmse_m1)):\n",
    "        rmse_mae_m1.append(rmse_m1[i])\n",
    "        rmse_mae_m1.append(mae_m1[i])\n",
    "        rmse_mae_m2.append(rmse_m2[i])\n",
    "        rmse_mae_m2.append(mae_m2[i])\n",
    "    labels = get_X_labels(org_X_test)\n",
    "\n",
    "    for i in range(org_y_test.shape[0]):\n",
    "        df_model[\"%s\" %(labels[i])] = np.concatenate((org_y_test[i],codes),axis=None)\n",
    "        df_model[\"%s-pre_m1\" %(labels[i])] = np.concatenate((predictions_m1[i], rmse_mae_m1[i*2:(i+1)*2]),axis=None)\n",
    "        df_model[\"%s-pre_m2\" %(labels[i])] = np.concatenate((predictions_m2[i], rmse_mae_m2[i*2:(i+1)*2]), axis=None)\n",
    "    df_model.to_excel(writer,'Run_%s'%(runs+1))\n",
    "    writer.save()\n",
    "    print(\"\\n\",\"Run_%s saved\"%(runs+1),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Auftrag  Filamentdichte  Drucker_1  Drucker_2  Drucker_3  Drucker_4  \\\n",
      "10681       71             100        0.0        0.0        1.0        0.0   \n",
      "1842         8              10        0.0        0.0        1.0        0.0   \n",
      "11204       73              55        0.0        0.0        0.0        0.0   \n",
      "\n",
      "       Drucker_5  Volumen  Quader  Kegel  Wirkleistung  \n",
      "10681        0.0     67.0     1.0    0.0     55.617600  \n",
      "1842         0.0    125.0     1.0    0.0     55.576960  \n",
      "11204        1.0    125.0     1.0    0.0    120.902276   \n",
      "\n",
      "\n",
      "#Trainingsdaten: 79\n",
      "#Testdaten: 9 \n",
      "\n",
      "Starting Run_1\n",
      "Training model_1 (appx. 15s)...\n",
      "Training model_2 (appx. 5min)........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      " Run_1 saved \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scale_mode = \"x_y_together\"\n",
    "\n",
    "for i in range(1):\n",
    "    all_data, longest_print = data_preparation() #geshuffelten daten nparray\n",
    "    X_train, X_test, y_train, y_test, org_X_test, org_y_test = normalization(all_data, scaler)\n",
    "    print(\"Starting Run_%s\"%(i+1))\n",
    "    to_excel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
