{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning approach for predicting power curves of manufacturing processes\n",
    "\n",
    "This Version (v1.0) was published alongside the homonymous CIRP paper: <a href=\"url\">link text</a>.\n",
    "The program is part of an on-going research project of the Laboratory for Machine Tools and Production Engineering WZL, RWTH Aachen University, Aachen, Germany. Further adaptions and other use cases are foreseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-5ua9{font-weight:bold;text-align:left}\n",
    ".tg .tg-s268{text-align:left !important;} \n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-5ua9\">ï»¿Name</th>\n",
    "    <th class=\"tg-5ua9\">Input</th>\n",
    "    <th class=\"tg-5ua9\">Output</th>\n",
    "    <th class=\"tg-5ua9\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">data_import_and_preparation</td>\n",
    "    <td class=\"tg-s268\">-</td>\n",
    "    <td class=\"tg-s268\">np.array of all measurements</td>\n",
    "    <td class=\"tg-s268\">import data from .csv file, one hot encoding, shuffle series of measurements</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">seperate_Xy</td>\n",
    "    <td class=\"tg-s268\">measurement series</td>\n",
    "    <td class=\"tg-s268\">Print settings (input), Energy measurements (output)</td>\n",
    "    <td class=\"tg-s268\">seperating input and output of the datafile</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">normalization</td>\n",
    "    <td class=\"tg-s268\">measurement series, scale mode</td>\n",
    "    <td class=\"tg-s268\">Normalized train- &amp; testdata, original input and output values</td>\n",
    "    <td class=\"tg-s268\">normalize the measurements to make the NN learn better</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">DNN_model</td>\n",
    "    <td class=\"tg-s268\">various NN settings</td>\n",
    "    <td class=\"tg-s268\">DNN model</td>\n",
    "    <td class=\"tg-s268\">creates an sequential model of the given input settings</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">train_dnn</td>\n",
    "    <td class=\"tg-s268\">model, X_train, y_train, epochs, verbose,NAME</td>\n",
    "    <td class=\"tg-s268\">model, history</td>\n",
    "    <td class=\"tg-s268\">trains the NN with the given settings</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">plot_model_history</td>\n",
    "    <td class=\"tg-s268\">history, NAME</td>\n",
    "    <td class=\"tg-s268\">-</td>\n",
    "    <td class=\"tg-s268\">plots the accuracy and loss values for train and validation data of the model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">descale</td>\n",
    "    <td class=\"tg-s268\">predictions, upper_limit</td>\n",
    "    <td class=\"tg-s268\">predictions</td>\n",
    "    <td class=\"tg-s268\">takes the predicted values of the noralized inputs and converts them back to the original scale</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">rmse_mae</td>\n",
    "    <td class=\"tg-s268\">predictions,y_test,lower_limit, upper_limit, return_single_values</td>\n",
    "    <td class=\"tg-s268\">rmses, maes</td>\n",
    "    <td class=\"tg-s268\">takes the predicted values and calculates the rmses and maes of each measurement.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">prediction_graph</td>\n",
    "    <td class=\"tg-s268\">predictions, lower_limit, upper_limit</td>\n",
    "    <td class=\"tg-s268\">-</td>\n",
    "    <td class=\"tg-s268\">plots predictions together with the original data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">get_X_labels</td>\n",
    "    <td class=\"tg-s268\">X_values</td>\n",
    "    <td class=\"tg-s268\">NAMES</td>\n",
    "    <td class=\"tg-s268\">Converts the one hot encoding back to normal job labels</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">model_1</td>\n",
    "    <td class=\"tg-s268\">input_layer_size,hidden_layers,hidden_layer_size,epochs,optimizer</td>\n",
    "    <td class=\"tg-s268\">model, history, prediction</td>\n",
    "    <td class=\"tg-s268\">creates an DNN via the integraded approach and makes test predictions</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">y_single_value_filter</td>\n",
    "    <td class=\"tg-s268\">y_data, zeitpunkt</td>\n",
    "    <td class=\"tg-s268\">y_single_value</td>\n",
    "    <td class=\"tg-s268\">return all measurements for a specific point of time</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">model_2</td>\n",
    "    <td class=\"tg-s268\">input_layer_size, hidden_layers, hidden_layer_size, epochs, optimizer, upper_limit</td>\n",
    "    <td class=\"tg-s268\">predictions_descaled</td>\n",
    "    <td class=\"tg-s268\">creates models for each measurement timestep and returns predictions on these models on the test data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-5ua9\">One Run --&gt; Excel</td>\n",
    "    <td class=\"tg-s268\">runse</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\">Makes Predictions on both models for shuffled data various times</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from math import exp\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras import metrics\n",
    "import warnings\n",
    "import time\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)  \n",
    "# Set global variables\n",
    "num_of_printers = 5\n",
    "y_position_in_csv, attributes_of_part = num_of_printers+5,num_of_printers+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import_and_preparation():\n",
    "    dataset = pd.read_csv('DruckerDaten_DOE_V6.csv', header=0, sep=';')\n",
    "    dataset.sample(5)\n",
    "    \n",
    "    origin = dataset.pop('Geometrie')\n",
    "    \n",
    "    dataset['Quader'] = (origin == 'Quader')*1.0\n",
    "    dataset['Kegel'] = (origin == 'Kegel')*1.0\n",
    "\n",
    "    origin = dataset.pop('DruckerID')\n",
    "    \n",
    "    for i in range(num_of_printers):\n",
    "        dataset['Drucker_%s'%(i+1)] = (origin == (i+1))*1.0\n",
    "\n",
    "    dataset.shape[0]\n",
    "    dataset = dataset[['Auftrag','Filamentdichte','Drucker_1','Drucker_2','Drucker_3','Drucker_4','Drucker_5','Volumen','Quader','Kegel', 'Wirkleistung']]\n",
    "    print(dataset.sample(3),\"\\n\\n\")\n",
    "    \n",
    "    def get_y(dataset):\n",
    "        messwert = []\n",
    "        y = []\n",
    "        for i in range(dataset.shape[0]):\n",
    "            def new_part ():\n",
    "                messwert.append(dataset.iloc[i,y_position_in_csv])\n",
    "            def add_to_parts():\n",
    "                y.append(messwert)\n",
    "\n",
    "            if i == 0:\n",
    "                new_part() \n",
    "            elif dataset.iloc[i, 0] == dataset.iloc[i-1, 0]:\n",
    "                messwert.append(dataset.iloc[i,y_position_in_csv])\n",
    "                if i == dataset.shape[0] - 1:\n",
    "                    add_to_parts()\n",
    "            else:\n",
    "                add_to_parts()\n",
    "                messwert = []\n",
    "                new_part()\n",
    "        return y\n",
    "        \n",
    "    y_data = get_y(dataset)\n",
    "\n",
    "    datenmenge = len(y_data)\n",
    "    longest_print = max(y_data, key=len)\n",
    "    longest_print = len(longest_print)\n",
    "\n",
    "    for y in y_data:\n",
    "        if len(y) < longest_print:\n",
    "            to_go = longest_print-len(y)\n",
    "            for z in range(to_go):\n",
    "                y.append(0)\n",
    "\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    X_dataset = dataset.drop(['Wirkleistung','Auftrag'], axis=1)\n",
    "    X_dataset = X_dataset.drop_duplicates()\n",
    "    X_data = np.array(X_dataset)\n",
    "    \n",
    "    all_data = X_data.tolist()\n",
    "    \n",
    "    for i in range(len(X_data)):\n",
    "        for k in range(len(y_data[i])):\n",
    "            all_data[i].append(y_data[i][k])\n",
    "            \n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    all_data = np.array(all_data, dtype=\"f\")\n",
    "    return all_data, longest_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_Xy(all_data):\n",
    "    X_data=[]\n",
    "    y_data =[]\n",
    "    for i in range(len(all_data)):\n",
    "        X_data.append(all_data[i][:y_position_in_csv-1])\n",
    "        y_data.append(all_data[i][y_position_in_csv-1:])\n",
    "    y_data = np.array(y_data)\n",
    "    X_data = np.array(X_data)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(all_data, scaler):\n",
    "    X_data, y_data = seperate_Xy(all_data)\n",
    "    number_of_datapoints = len(X_data)\n",
    "    \n",
    "    if scale_mode == \"x_y_seperate\":\n",
    "        scaled_X_data  = scaler.fit_transform(X_data)\n",
    "        scaled_y_data  = scaler.fit_transform(y_data) \n",
    "\n",
    "    if scale_mode == \"x_y_together\": \n",
    "        all_scaled_data = scaler.fit_transform(all_data.transpose())\n",
    "        all_scaled_data = all_scaled_data.transpose()\n",
    "        scaled_X_data, scaled_y_data = seperate_Xy(all_scaled_data)\n",
    "\n",
    "    if scale_mode == \"x_only\":\n",
    "        scaled_X_data = scaler.fit_transform(X_data)\n",
    "        scaled_y_data = y_data\n",
    "\n",
    "    number_of_datapoints = len(X_data)\n",
    "\n",
    "    org_y_test = np.array(y_data[round(0.9*number_of_datapoints):]) #10 Prozent testdaten, 90 Prozent zum Trainieren des Models\n",
    "    org_X_test = np.array(X_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    X_train = np.array(scaled_X_data[:round(0.9*number_of_datapoints)])\n",
    "    X_test = np.array(scaled_X_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    y_train = np.array(scaled_y_data[:round(0.9*number_of_datapoints)])\n",
    "    y_test = np.array(scaled_y_data[round(0.9*number_of_datapoints):])\n",
    "\n",
    "    print(\"#Trainingsdaten:\",X_train.shape[0])\n",
    "    print(\"#Testdaten:\",X_test.shape[0], \"\\n\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, org_X_test, org_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#Open a console, change to your working directory, and type: tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(input_layer_size, dense_layers, hidden_layer_size, output_layer_size, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_size, activation='relu',input_shape = (X_train.shape[1:])))\n",
    "    \n",
    "    for l in range(dense_layers):\n",
    "        model.add(Dense(hidden_layer_size, activation= 'relu'))\n",
    "        #model.add(BatchNormalization(axis=1))\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_layer_size))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer = optimizer, \n",
    "                  metrics=['accuracy','mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DNN(model, X_train, y_train, epochs, verbose,NAME):\n",
    "    \n",
    "    tbCallBack = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size= (round(X_train.shape[0]/3)),\n",
    "                        validation_split = 0.1,\n",
    "                        epochs = epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=True,\n",
    "                        callbacks = [tbCallBack])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history, NAME):\n",
    "    pyplot.figure(100)\n",
    "    pyplot.plot(history.history['loss'], label='training')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.title(NAME)\n",
    "    pyplot.figure(101)\n",
    "    pyplot.plot(history.history['acc'], label='training')\n",
    "    pyplot.plot(history.history['val_acc'], label='validation')\n",
    "    pyplot.legend()  \n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(predictions, upper_limit):\n",
    "    if scale_mode in [\"x_y_seperate\"]:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "    if scale_mode == \"x_y_together\":\n",
    "        predictions = np.concatenate((X_test, predictions), axis = 1) \n",
    "        all_data_but_pred = np.concatenate((X_train, y_train[:,:upper_limit]), axis = 1)\n",
    "        all_data_but_pred = np.concatenate((all_data_but_pred, predictions), axis =0)\n",
    "        test_predictions_rescaled = scaler.inverse_transform((all_data_but_pred.transpose()))\n",
    "        test_predictions_rescaled = test_predictions_rescaled.transpose()\n",
    "        predictions = test_predictions_rescaled[X_train.shape[0]:,attributes_of_part:] \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_mae(predictions,y_test,lower_limit, upper_limit, return_single_values):\n",
    "    rmses, maes = [], []\n",
    "\n",
    "    if return_single_values == True:\n",
    "        for i in range(predictions.shape[0]):\n",
    "            rmse = sqrt(mean_squared_error(predictions[i], org_y_test[i,lower_limit:upper_limit]))\n",
    "            mae = mean_absolute_error(predictions[i], org_y_test[i,lower_limit:upper_limit])\n",
    "            rmses.append(rmse)\n",
    "            maes.append(mae)  \n",
    "    if return_single_values == False:        \n",
    "        rmses = sqrt(mean_squared_error(predictions, org_y_test[:,lower_limit:upper_limit]))\n",
    "        maes = mean_absolute_error(predictions, org_y_test[:,lower_limit:upper_limit])\n",
    "    return rmses, maes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_graph(predictions, lower_limit, upper_limit):\n",
    "    for i in range(len(y_test)):\n",
    "        pyplot.figure(i)\n",
    "        pyplot.plot(predictions[i], 'r',label='Vorhersage')\n",
    "        pyplot.plot(org_y_test[i,lower_limit:upper_limit],label='Leistungskurve')\n",
    "        pyplot.legend()\n",
    "        pyplot.ylabel('Wirkleistung [W]')\n",
    "        pyplot.xlabel('Zeitschritte [min]')\n",
    "        pyplot.title(org_X_test[i])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_labels(X_values):\n",
    "    NAMES = []\n",
    "    for i in range(X_values.shape[0]):\n",
    "        for j in range(num_of_printers):\n",
    "            if X_values[i,j+1] == 1:\n",
    "                printer = \"Printer%s\"%(j+1)\n",
    "        if X_values[i,num_of_printers+2] == 1:\n",
    "            geometry = \"Quader\"\n",
    "        else:\n",
    "            geometry = \"Kegel\"\n",
    "        NAME = \"%s %s F%s V%s\" %(printer,geometry,X_values[i,0],X_values[i,num_of_printers+1])\n",
    "        NAMES.append(NAME)\n",
    "    return NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_01 - Integrated Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1 (input_layer_size,hidden_layers,hidden_layer_size,epochs,optimizer):\n",
    "    NAME = \"%s-inlay_%s-hlayers_%s-hsize_%s\"%(input_layer_size,hidden_layers,hidden_layer_size,int(time.time()))\n",
    "    lower_limit, upper_limit = 0, longest_print\n",
    "    input_layer_size = input_layer_size\n",
    "    hidden_layers = hidden_layers\n",
    "    hidden_layer_size = hidden_layer_size\n",
    "    output_layer_size = longest_print\n",
    "    optimizer = optimizer\n",
    "    batch_size = [round(X_train.shape[0]/3)]\n",
    "    verbose = 0\n",
    "    epochs = epochs\n",
    "    \n",
    "    print(\"Training model_1 (appx. 15s)...\")\n",
    "    model = DNN_model(input_layer_size, hidden_layers, hidden_layer_size, output_layer_size,optimizer)\n",
    "    model, history = train_DNN(model,X_train, y_train, epochs, verbose,NAME)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction = descale(prediction, upper_limit) \n",
    "    \n",
    "    return model, history, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_02 - Multi Net Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_single_value_filter(y_data, timestep):\n",
    "    y_single_value = y_data.transpose()[timestep]\n",
    "    y_single_value = np.array([y_single_value])\n",
    "    y_single_value = y_single_value.transpose()\n",
    "    return y_single_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_02(model,X_train, y_train, epochs, verbose):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=(round(X_train.shape[0]/3)),\n",
    "                        validation_split = 0.1,\n",
    "                        epochs = epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=True)\n",
    "    prediction = model.predict(X_test)\n",
    "    return prediction, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(input_layer_size, hidden_layers, hidden_layer_size, epochs, optimizer, upper_limit): \n",
    "    NAME = \"%s-inlay_%s-hlayers_%s-hsize_%s\"%(input_layer_size,hidden_layers,hidden_layer_size,int(time.time()))\n",
    "    lower_limit, upper_limit = 0, upper_limit\n",
    "    input_layer_size = input_layer_size\n",
    "    hidden_layers = hidden_layers\n",
    "    hidden_layer_size = hidden_layer_size\n",
    "    output_layer_size = 1\n",
    "    optimizer = optimizer\n",
    "    epochs = epochs\n",
    "    verbose = 0\n",
    "    \n",
    "    model = DNN_model (input_layer_size, hidden_layers, hidden_layer_size, output_layer_size, optimizer)\n",
    "    predictions = np.array([[0]]*X_test.shape[0]) \n",
    "    print(\"Training model_2 (appx. 5min)\",end=\"\")\n",
    "    for y_stelle in range(lower_limit,upper_limit,1): \n",
    "        print(\".\", end=\"\")\n",
    "        y_single_train = y_single_value_filter(y_train,y_stelle)\n",
    "        y_single_test = y_single_value_filter(y_test, y_stelle)\n",
    "\n",
    "        model, history = train_DNN(model, X_train, y_single_train,epochs, verbose,NAME)\n",
    "        prediction = model.predict(X_test)\n",
    "        predictions = np.concatenate((predictions, prediction), axis=1)\n",
    "\n",
    "    predictions = np.delete(predictions,0,1) \n",
    "    predictions_descaled = descale(predictions, upper_limit)\n",
    "\n",
    "    return predictions_descaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Run -->  Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_excel(runs):\n",
    "    \n",
    "    from threading import Thread\n",
    "\n",
    "    keras.optimizers.Adam(lr=0.0001)\n",
    "    keras.optimizers.RMSprop(lr=0.001)\n",
    "    batch_size = (round(X_train.shape[0]/3))\n",
    "    optimizer = 'adam'\n",
    "    codes = [666,999]\n",
    "    writer = pd.ExcelWriter('run_%s.xlsx'%(runs+1))\n",
    "    df_model = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    model_m1, history_m1, predictions_m1 = model_1(16,2,400,1000, optimizer)\n",
    "    rmse_m1, mae_m1 = rmse_mae(predictions_m1, y_test,0, longest_print, True) \n",
    "    rmse_mae_m1 = []\n",
    "    \n",
    "    predictions_m2 = model_2(64,1,64,100, optimizer, longest_print)\n",
    "    rmse_m2, mae_m2 = rmse_mae(predictions_m2, y_test, 0, longest_print, True)\n",
    "    rmse_mae_m2 = []\n",
    "    \n",
    "    for i in range(len(rmse_m1)):\n",
    "        rmse_mae_m1.append(rmse_m1[i])\n",
    "        rmse_mae_m1.append(mae_m1[i])\n",
    "        rmse_mae_m2.append(rmse_m2[i])\n",
    "        rmse_mae_m2.append(mae_m2[i])\n",
    "    labels = get_X_labels(org_X_test)\n",
    "\n",
    "    for i in range(org_y_test.shape[0]):\n",
    "        df_model[\"%s\" %(labels[i])] = np.concatenate((org_y_test[i],codes),axis=None)\n",
    "        df_model[\"%s-pre_m1\" %(labels[i])] = np.concatenate((predictions_m1[i], rmse_mae_m1[i*2:(i+1)*2]),axis=None)\n",
    "        df_model[\"%s-pre_m2\" %(labels[i])] = np.concatenate((predictions_m2[i], rmse_mae_m2[i*2:(i+1)*2]), axis=None)\n",
    "    df_model.to_excel(writer,'Run_%s'%(runs+1))\n",
    "    writer.save()\n",
    "    print(\"Run_%s saved\"%(runs+1),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Auftrag  Filamentdichte  Drucker_1  Drucker_2  Drucker_3  Drucker_4  \\\n",
      "6560       39             100        0.0        0.0        0.0        1.0   \n",
      "9516       61              10        0.0        0.0        0.0        1.0   \n",
      "8659       53              10        0.0        0.0        1.0        0.0   \n",
      "\n",
      "      Drucker_5  Volumen  Quader  Kegel  Wirkleistung  \n",
      "6560        0.0  125.000     1.0    0.0     60.565895  \n",
      "9516        0.0   67.000     1.0    0.0     63.602010  \n",
      "8659        0.0   19.635     0.0    1.0     56.347512   \n",
      "\n",
      "\n",
      "#Trainingsdaten: 79\n",
      "#Testdaten: 9 \n",
      "\n",
      "Starting Run_1\n",
      "Training model_1...\n",
      "Training model_2:........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b69ca66821c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Run_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d668b6a22201>\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(runs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mrmse_mae_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpredictions_m2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mrmse_m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_m2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse_mae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest_print\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mrmse_mae_m2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8044c484cb8d>\u001b[0m in \u001b[0;36mmodel_2\u001b[0;34m(input_layer_size, hidden_layers, hidden_layer_size, epochs, optimizer, upper_limit)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my_single_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_single_value_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stelle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_DNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_single_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-be1df9b1e375>\u001b[0m in \u001b[0;36mtrain_DNN\u001b[0;34m(model, X_train, y_train, epochs, verbose, NAME)\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks = [tbCallBack])\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m               \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             if k not in ['batch', 'size', 'num_steps']}\n\u001b[0;32m-> 1102\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_custom_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# pop the histogram summary op after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_write_custom_summaries\u001b[0;34m(self, step, logs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0msummary_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0msummary_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m         \u001b[0msummary_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scale_mode = \"x_y_together\"\n",
    "\n",
    "for i in range(1):\n",
    "    all_data, longest_print = data_import_and_preparation() #geshuffelten daten nparray\n",
    "    X_train, X_test, y_train, y_test, org_X_test, org_y_test = normalization(all_data, scaler)\n",
    "    print(\"Starting Run_%s\"%(i+1))\n",
    "    to_excel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
